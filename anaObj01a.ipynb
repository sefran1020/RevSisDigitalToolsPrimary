{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3359306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory 'ResObj01a' created or already exists.\n",
      "Starting analysis for Objective 1...\n",
      "Dataset 'analisisTodos.csv' loaded successfully. Shape: (29, 28)\n",
      "Data preparation complete. Working with 29 studies.\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Regex error in pattern '(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)': look-behind requires fixed-width pattern\n",
      "Generating Summary Dashboard...\n",
      "Summary Dashboard generated successfully.\n",
      "\n",
      "Comprehensive analysis report saved successfully to: ResObj01a\\Objective1_Analysis_Report.txt\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE. All generated outputs saved in the 'ResObj01a' directory.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "STATISTICAL ANALYSIS FOR SYSTEMATIC REVIEW - OBJECTIVE 1\n",
    "Examining the Impact of Interactive Digital Tools on Mathematical Skills and Computational Thinking\n",
    "in Primary Education Students (6-12 years)\n",
    "\n",
    "Author: [Your Name] - Corrected and Refactored by AI Assistant\n",
    "Version: 1.1\n",
    "\"\"\"\n",
    "\n",
    "# --- 0. Import Required Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats # For statistical tests\n",
    "import re             # For regular expressions (text processing)\n",
    "import os             # For interacting with the operating system (creating folders)\n",
    "import networkx as nx # For network analysis\n",
    "from wordcloud import WordCloud # For creating word clouds\n",
    "import nltk           # Natural Language Toolkit for text processing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # For TF-IDF analysis\n",
    "import warnings       # To suppress warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set up output directory (Changed to ResObj01a as requested)\n",
    "output_dir = 'ResObj01a'\n",
    "try:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Output directory '{output_dir}' created or already exists.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error creating directory {output_dir}: {e}\")\n",
    "    # Exit if directory creation fails, as outputs cannot be saved\n",
    "    exit()\n",
    "\n",
    "# Configure visualization aesthetics\n",
    "plt.style.use('seaborn-v0_8-whitegrid') # Use a visually appealing style\n",
    "sns.set_palette(\"viridis\")              # Set a consistent color palette\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans' # Ensure consistent font\n",
    "plt.rcParams['figure.figsize'] = (12, 8)    # Set default figure size\n",
    "plt.rcParams['figure.dpi'] = 100            # Set default figure resolution\n",
    "\n",
    "# Make sure NLTK resources (punkt tokenizer, stopwords) are available\n",
    "# Download them quietly if not found\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"NLTK 'punkt' resource not found. Downloading...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"NLTK 'stopwords' resource not found. Downloading...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# --- Report Initialization ---\n",
    "report_lines = []\n",
    "report_lines.append(\"=== SYSTEMATIC REVIEW ANALYSIS REPORT: OBJECTIVE 1 ===\")\n",
    "report_lines.append(\"Impact of Digital Interactive Tools on Mathematical Skills and Computational Thinking\")\n",
    "report_lines.append(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\") # Added time\n",
    "report_lines.append(\"=\" * 60)\n",
    "\n",
    "print(\"Starting analysis for Objective 1...\")\n",
    "\n",
    "# --- 1. Data Loading and Preparation ---\n",
    "report_lines.append(\"\\n1. DATA LOADING AND PREPARATION\")\n",
    "report_lines.append(\"-\" * 40)\n",
    "\n",
    "data_file = 'analisisTodos.csv'\n",
    "try:\n",
    "    # Load the CSV file (assuming semicolon delimiter and utf-8 encoding)\n",
    "    df = pd.read_csv(data_file, delimiter=';', encoding='utf-8')\n",
    "    print(f\"Dataset '{data_file}' loaded successfully. Shape: {df.shape}\")\n",
    "    report_lines.append(f\"Dataset loaded with {df.shape[0]} records and {df.shape[1]} columns.\")\n",
    "except FileNotFoundError:\n",
    "    error_msg = f\"Error: Dataset file '{data_file}' not found. Please ensure it's in the same directory.\"\n",
    "    print(error_msg)\n",
    "    report_lines.append(error_msg)\n",
    "    exit() # Exit if the data file is missing\n",
    "except Exception as e:\n",
    "    error_msg = f\"Error loading the dataset: {str(e)}\"\n",
    "    print(error_msg)\n",
    "    report_lines.append(error_msg)\n",
    "    exit() # Exit on other loading errors\n",
    "\n",
    "# Define relevant columns for Objective 1\n",
    "objective1_cols = [\n",
    "    'Authors', 'Year', 'Country', 'Sample Size', 'Study Design',\n",
    "    'Intervention', 'Control/Comparison', 'Effect Sizes',\n",
    "    'General Results',\n",
    "    'Efectividad Cognitiva: Desarrollo de Habilidades Matemáticas',\n",
    "    'Efectividad Cognitiva: Resolución de Problemas y Pensamiento Computacional',\n",
    "    'Herramientas Digitales: Tipos de Herramientas Interactivas'\n",
    "]\n",
    "\n",
    "# Check if all required columns exist in the loaded dataframe\n",
    "missing_cols = [col for col in objective1_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    warning_msg = f\"Warning: The following expected columns are missing: {', '.join(missing_cols)}. Analysis will proceed with available columns.\"\n",
    "    print(warning_msg)\n",
    "    report_lines.append(warning_msg)\n",
    "    # Filter objective1_cols to only include columns that actually exist\n",
    "    objective1_cols = [col for col in objective1_cols if col in df.columns]\n",
    "    if not objective1_cols:\n",
    "        error_msg = \"Error: No relevant columns found for Objective 1 analysis. Exiting.\"\n",
    "        print(error_msg)\n",
    "        report_lines.append(error_msg)\n",
    "        exit()\n",
    "\n",
    "# Create the working dataframe with only the relevant (and existing) columns\n",
    "df_obj1 = df[objective1_cols].copy()\n",
    "\n",
    "# Rename columns for easier programmatic access (using snake_case)\n",
    "column_mapping = {\n",
    "    'Authors': 'authors',\n",
    "    'Year': 'year',\n",
    "    'Country': 'country',\n",
    "    'Sample Size': 'sample_size',\n",
    "    'Study Design': 'study_design',\n",
    "    'Intervention': 'intervention',\n",
    "    'Control/Comparison': 'control',\n",
    "    'Effect Sizes': 'effect_sizes',\n",
    "    'General Results': 'general_results',\n",
    "    # Handling potentially missing columns gracefully during rename\n",
    "    'Efectividad Cognitiva: Desarrollo de Habilidades Matemáticas': 'math_skills',\n",
    "    'Efectividad Cognitiva: Resolución de Problemas y Pensamiento Computacional': 'problem_solving',\n",
    "    'Herramientas Digitales: Tipos de Herramientas Interactivas': 'digital_tools'\n",
    "}\n",
    "\n",
    "# Apply renaming only for columns that exist in df_obj1\n",
    "actual_mapping = {k: v for k, v in column_mapping.items() if k in df_obj1.columns}\n",
    "df_obj1.rename(columns=actual_mapping, inplace=True)\n",
    "\n",
    "# Create a combined cognitive text field for easier text analysis\n",
    "# Fill NaN values with empty strings before concatenation\n",
    "cognitive_cols_present = [col for col in ['math_skills', 'problem_solving'] if col in df_obj1.columns]\n",
    "if cognitive_cols_present:\n",
    "    df_obj1['cognitive_text'] = df_obj1[cognitive_cols_present].fillna('').agg(' '.join, axis=1)\n",
    "else:\n",
    "    df_obj1['cognitive_text'] = '' # Create empty column if source cols are missing\n",
    "    report_lines.append(\"Warning: 'math_skills' or 'problem_solving' columns missing. 'cognitive_text' will be empty.\")\n",
    "\n",
    "# --- Data Cleaning Functions ---\n",
    "def extract_sample_size(text):\n",
    "    \"\"\"Extracts the first number found in the sample size text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    # Find all sequences of digits\n",
    "    numbers = re.findall(r'\\d+', str(text))\n",
    "    # Return the first number found, or NaN if none are found\n",
    "    return int(numbers[0]) if numbers else np.nan\n",
    "\n",
    "# --- Apply Cleaning ---\n",
    "if 'sample_size' in df_obj1.columns:\n",
    "    df_obj1['numeric_sample_size'] = df_obj1['sample_size'].apply(extract_sample_size)\n",
    "else:\n",
    "    df_obj1['numeric_sample_size'] = np.nan # Create column with NaNs if source is missing\n",
    "    report_lines.append(\"Warning: 'sample_size' column missing. Cannot extract numeric sample sizes.\")\n",
    "\n",
    "# Convert 'year' to numeric, coercing errors to NaN\n",
    "if 'year' in df_obj1.columns:\n",
    "    df_obj1['year'] = pd.to_numeric(df_obj1['year'], errors='coerce')\n",
    "    # Drop rows where year could not be parsed (optional, but good practice)\n",
    "    initial_rows = len(df_obj1)\n",
    "    df_obj1.dropna(subset=['year'], inplace=True)\n",
    "    if len(df_obj1) < initial_rows:\n",
    "        print(f\"Dropped {initial_rows - len(df_obj1)} rows due to invalid 'Year' entries.\")\n",
    "    df_obj1['year'] = df_obj1['year'].astype(int) # Convert valid years to integer\n",
    "    min_year = df_obj1['year'].min()\n",
    "    max_year = df_obj1['year'].max() # Corrected from max3\n",
    "    report_lines.append(f\"Years covered in the valid data: {min_year} to {max_year}\")\n",
    "else:\n",
    "    report_lines.append(\"Warning: 'year' column missing. Temporal analysis will not be possible.\")\n",
    "    min_year, max_year = None, None # Set to None if year column is missing\n",
    "\n",
    "report_lines.append(f\"Working dataset created with {len(df_obj1)} valid studies after initial cleaning.\")\n",
    "print(f\"Data preparation complete. Working with {len(df_obj1)} studies.\")\n",
    "\n",
    "# --- 2. Effect Size Analysis ---\n",
    "report_lines.append(\"\\n2. EFFECT SIZE ANALYSIS\")\n",
    "report_lines.append(\"-\" * 40)\n",
    "\n",
    "def extract_effect_sizes(text):\n",
    "    \"\"\"Extracts various effect size metrics (d, g, eta^2, r) from text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    # Define regex patterns for common effect size notations\n",
    "    # Using non-capturing groups (?:...) and clear boundaries\n",
    "    patterns = [\n",
    "        r\"Cohen's\\s+d\\s*=\\s*(-?\\d*\\.?\\d+)\",          # Cohen's d = 0.5\n",
    "        r\"(?<!Cohen's\\s)d\\s*=\\s*(-?\\d*\\.?\\d+)\",     # d = .5 (avoid matching 'sd =')\n",
    "        r\"Hedges'?\\s+g\\s*=\\s*(-?\\d*\\.?\\d+)\",         # Hedges g = 0.45 or Hedges' g = 0.45\n",
    "        r\"(?<!Hedges'?\\s)g\\s*=\\s*(-?\\d*\\.?\\d+)\",    # g = .45 (avoid matching 'mg =')\n",
    "        r\"(?:partial\\s+)?eta\\s+squared\\s*=\\s*(\\d*\\.?\\d+)\", # eta squared = 0.1, partial eta squared = .1\n",
    "        r\"ηp?²\\s*=\\s*(\\d*\\.?\\d+)\",                   # η² = 0.1, ηp² = .1 (only positive values)\n",
    "        r\"(?<![a-zA-Z])r\\s*=\\s*(-?\\d*\\.?\\d+)\",       # r = 0.3 (correlation) - ensure 'r' is not part of a word\n",
    "        r\"(-?\\d*\\.?\\d+)σ\",                          # e.g., -0.30σ (standard deviation units)\n",
    "    ]\n",
    "\n",
    "    text_processed = str(text).replace(',', '.')  # Standardize decimal separator\n",
    "    effect_sizes = []\n",
    "\n",
    "    for pattern in patterns:\n",
    "        try:\n",
    "            matches = re.findall(pattern, text_processed, re.IGNORECASE)\n",
    "            for value_str in matches:\n",
    "                try:\n",
    "                    # Convert matched string to float\n",
    "                    value_float = float(value_str)\n",
    "                    # Basic plausibility check (e.g., very large d or g might be errors)\n",
    "                    # Adjust range as needed based on expected values\n",
    "                    if abs(value_float) < 10: # Avoid extremely large values often due to typos\n",
    "                         effect_sizes.append(value_float)\n",
    "                except ValueError:\n",
    "                    continue # Ignore if conversion fails\n",
    "        except re.error as e:\n",
    "            print(f\"Regex error in pattern '{pattern}': {e}\")\n",
    "            continue # Skip pattern if it's invalid\n",
    "\n",
    "    # Return unique effect sizes found\n",
    "    return list(set(effect_sizes))\n",
    "\n",
    "# Extract effect sizes from potentially relevant columns (if they exist)\n",
    "es_cols = ['effect_sizes', 'general_results', 'cognitive_text']\n",
    "for col in es_cols:\n",
    "    if col in df_obj1.columns:\n",
    "        df_obj1[f'es_from_{col}'] = df_obj1[col].apply(extract_effect_sizes)\n",
    "    else:\n",
    "        df_obj1[f'es_from_{col}'] = [[] for _ in range(len(df_obj1))] # Empty lists if col missing\n",
    "\n",
    "# Combine all extracted effect sizes into a single list per study\n",
    "df_obj1['all_effect_sizes'] = df_obj1[[f'es_from_{col}' for col in es_cols]].sum(axis=1)\n",
    "df_obj1['all_effect_sizes'] = df_obj1['all_effect_sizes'].apply(lambda x: sorted(list(set(x)))) # Unique & sorted\n",
    "\n",
    "# Create metrics based on extracted effect sizes\n",
    "df_obj1['has_effect_size'] = df_obj1['all_effect_sizes'].apply(lambda x: len(x) > 0)\n",
    "df_obj1['num_effect_sizes'] = df_obj1['all_effect_sizes'].apply(len)\n",
    "\n",
    "# Calculate mean *absolute* effect size per study (common practice for magnitude)\n",
    "# Handle cases with no effect sizes (results in NaN)\n",
    "df_obj1['mean_effect_size'] = df_obj1['all_effect_sizes'].apply(\n",
    "    lambda x: np.mean([abs(val) for val in x]) if x else np.nan\n",
    ")\n",
    "\n",
    "# Filter to get studies that reported at least one effect size\n",
    "df_with_es = df_obj1[df_obj1['has_effect_size']].copy()\n",
    "num_studies_with_es = len(df_with_es)\n",
    "# Flatten the list of all individual effect sizes found across studies\n",
    "all_individual_es = [es for sublist in df_with_es['all_effect_sizes'] for es in sublist if pd.notna(es)]\n",
    "\n",
    "report_lines.append(f\"Studies with extractable effect sizes: {num_studies_with_es} out of {len(df_obj1)}\")\n",
    "if all_individual_es:\n",
    "    report_lines.append(f\"Total individual effect size values extracted: {len(all_individual_es)}\")\n",
    "else:\n",
    "     report_lines.append(\"No individual effect size values were successfully extracted.\")\n",
    "\n",
    "mean_es, median_es = np.nan, np.nan # Initialize as NaN\n",
    "\n",
    "if num_studies_with_es > 0 and not df_with_es['mean_effect_size'].isna().all():\n",
    "    # Calculate overall mean and median effect size across studies\n",
    "    mean_es = df_with_es['mean_effect_size'].mean()\n",
    "    median_es = df_with_es['mean_effect_size'].median()\n",
    "    min_mean_es = df_with_es['mean_effect_size'].min()\n",
    "    max_mean_es = df_with_es['mean_effect_size'].max()\n",
    "\n",
    "    report_lines.append(f\"Mean absolute effect size across studies: {mean_es:.3f}\")\n",
    "    report_lines.append(f\"Median absolute effect size across studies: {median_es:.3f}\")\n",
    "    report_lines.append(f\"Range of mean absolute effect sizes: {min_mean_es:.3f} to {max_mean_es:.3f}\")\n",
    "\n",
    "    # Categorize mean effect sizes based on Cohen's guidelines (absolute values)\n",
    "    # Bins: (-inf, 0.2), [0.2, 0.5), [0.5, 0.8), [0.8, inf)\n",
    "    bins = [-np.inf, 0.2, 0.5, 0.8, np.inf]\n",
    "    labels = ['Small (<0.2)', 'Medium (0.2-0.5)', 'Large (0.5-0.8)', 'Very Large (>0.8)']\n",
    "    df_with_es['effect_size_category'] = pd.cut(\n",
    "        df_with_es['mean_effect_size'],\n",
    "        bins=bins,\n",
    "        labels=labels,\n",
    "        right=False # Intervals are [min, max)\n",
    "    )\n",
    "\n",
    "    # Calculate counts for each effect size category\n",
    "    es_category_counts = df_with_es['effect_size_category'].value_counts().sort_index()\n",
    "    report_lines.append(\"\\nEffect size distribution by category (based on mean absolute ES per study):\")\n",
    "    for category, count in es_category_counts.items():\n",
    "        percentage = (count / num_studies_with_es) * 100\n",
    "        report_lines.append(f\"  {category}: {count} studies ({percentage:.1f}%)\")\n",
    "\n",
    "    # --- Visualize Effect Size Distribution ---\n",
    "    try:\n",
    "        fig_es, axs = plt.subplots(2, 2, figsize=(16, 12)) # Adjusted size\n",
    "        fig_es.suptitle(\"Effect Size Analysis\", fontsize=16, y=0.99) # Adjusted title pos\n",
    "\n",
    "        # Plot 1: Histogram of all individual effect sizes (raw values)\n",
    "        if all_individual_es:\n",
    "            sns.histplot(all_individual_es, kde=True, ax=axs[0, 0], bins=20)\n",
    "            axs[0, 0].set_title('Distribution of All Individual Effect Sizes (Raw)')\n",
    "            axs[0, 0].set_xlabel('Effect Size Value')\n",
    "            axs[0, 0].set_ylabel('Frequency')\n",
    "        else:\n",
    "            axs[0, 0].text(0.5, 0.5, 'No individual ES data', ha='center', va='center')\n",
    "            axs[0, 0].set_title('Distribution of All Individual Effect Sizes (Raw)')\n",
    "\n",
    "\n",
    "        # Plot 2: Histogram of mean absolute effect sizes per study\n",
    "        sns.histplot(df_with_es['mean_effect_size'].dropna(), kde=True, ax=axs[0, 1], bins=15)\n",
    "        axs[0, 1].set_title('Distribution of Mean Absolute Effect Size per Study')\n",
    "        axs[0, 1].set_xlabel('Mean Absolute Effect Size')\n",
    "        axs[0, 1].set_ylabel('Number of Studies')\n",
    "        axs[0, 1].axvline(mean_es, color='r', linestyle='--', label=f'Mean = {mean_es:.2f}')\n",
    "        axs[0, 1].axvline(median_es, color='g', linestyle=':', label=f'Median = {median_es:.2f}')\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        # Plot 3: Bar chart of effect size categories\n",
    "        if not es_category_counts.empty:\n",
    "            sns.barplot(x=es_category_counts.index, y=es_category_counts.values, ax=axs[1, 0], palette='viridis')\n",
    "            axs[1, 0].set_title('Effect Size Categories (Cohen\\'s Interpretation)')\n",
    "            axs[1, 0].set_xlabel('Category (Based on Mean Absolute ES)')\n",
    "            axs[1, 0].set_ylabel('Number of Studies')\n",
    "            axs[1, 0].tick_params(axis='x', rotation=30) # Slightly rotate labels\n",
    "        else:\n",
    "            axs[1, 0].text(0.5, 0.5, 'No category data', ha='center', va='center')\n",
    "            axs[1, 0].set_title('Effect Size Categories')\n",
    "\n",
    "\n",
    "        # Plot 4: Scatterplot of effect size vs. year (if 'year' exists and enough data)\n",
    "        if 'year' in df_with_es.columns and len(df_with_es.dropna(subset=['mean_effect_size', 'year'])) > 3:\n",
    "            sns.regplot(\n",
    "                x='year',\n",
    "                y='mean_effect_size',\n",
    "                data=df_with_es,\n",
    "                scatter_kws={'alpha': 0.6},\n",
    "                line_kws={'color': 'red'},\n",
    "                ax=axs[1, 1]\n",
    "            )\n",
    "            axs[1, 1].set_title('Mean Absolute Effect Size Trends by Year')\n",
    "            axs[1, 1].set_xlabel('Year')\n",
    "            axs[1, 1].set_ylabel('Mean Absolute Effect Size')\n",
    "            # Calculate and display correlation\n",
    "            corr, p_val = stats.pearsonr(df_with_es['year'].dropna(), df_with_es['mean_effect_size'].dropna())\n",
    "            axs[1, 1].annotate(f'r = {corr:.2f}\\np = {p_val:.2f}', xy=(0.05, 0.9), xycoords='axes fraction')\n",
    "        else:\n",
    "            axs[1, 1].text(0.5, 0.5, 'Insufficient data for trend analysis\\n(need >3 studies with Year and ES)',\n",
    "                           ha='center', va='center', fontsize=10)\n",
    "            axs[1, 1].set_title('Mean Absolute Effect Size Trends by Year')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout\n",
    "        es_fig_path = os.path.join(output_dir, 'effect_size_analysis.png')\n",
    "        plt.savefig(es_fig_path, dpi=100, bbox_inches='tight')\n",
    "        plt.close(fig_es) # Close the figure to free memory\n",
    "        report_lines.append(f\"Effect size analysis visualization saved to: {es_fig_path}\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error creating effect size visualization: {e}\"\n",
    "        print(error_msg)\n",
    "        report_lines.append(error_msg)\n",
    "\n",
    "else:\n",
    "    report_lines.append(\"Insufficient effect size data for detailed analysis or visualization.\")\n",
    "\n",
    "# --- 3. Intervention Type Analysis ---\n",
    "report_lines.append(\"\\n3. INTERVENTION TYPE ANALYSIS\")\n",
    "report_lines.append(\"-\" * 40)\n",
    "\n",
    "def categorize_intervention(text):\n",
    "    \"\"\"Categorizes intervention based on keywords in the description.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 'Unknown/Not Specified'\n",
    "\n",
    "    text_lower = str(text).lower() # Ensure lowercase string\n",
    "\n",
    "    # Define keywords for categories (prioritized order)\n",
    "    categories = {\n",
    "        'AR/VR': ['ar', 'vr', 'augment', 'virtual', 'realidad aumentada', 'realidad virtual', 'mixed reality'],\n",
    "        'Game-based Learning': ['game', 'gamif', 'juego', 'serious game'],\n",
    "        'Intelligent Tutor/Adaptive': ['tutor inteligente', 'intelligent tutor', 'its', 'adaptive', 'adaptativo', 'personalized learning'],\n",
    "        'Simulation/Modeling': ['simulation', 'simulación', 'modelado', 'virtual lab'],\n",
    "        'Robotics/Physical Computing': ['robot', 'robotics', 'physical computing', 'makeblock', 'lego mindstorms', 'scratch hardware'],\n",
    "        'Software/App/Platform': ['software', 'app', 'platform', 'plataforma', 'programa', 'web-based', 'online tool'],\n",
    "        'Interactive Classroom Tools': ['interactive whiteboard', 'pizarra interactiva', 'clicker', 'student response system'],\n",
    "        'Programming/Coding Env': ['programming', 'coding', 'scratch', 'blockly', 'code.org'], # Added specific category\n",
    "    }\n",
    "\n",
    "    for category, keywords in categories.items():\n",
    "        if any(re.search(r'\\b' + re.escape(kw) + r'\\b', text_lower) for kw in keywords):\n",
    "            return category\n",
    "\n",
    "    # Fallback categories\n",
    "    if 'interactive' in text_lower or 'interactivo' in text_lower:\n",
    "        return 'Interactive (General)'\n",
    "    else:\n",
    "        return 'Other/Mixed'\n",
    "\n",
    "# Apply categorization (check if 'intervention' column exists)\n",
    "if 'intervention' in df_obj1.columns:\n",
    "    df_obj1['intervention_type'] = df_obj1['intervention'].apply(categorize_intervention)\n",
    "else:\n",
    "    df_obj1['intervention_type'] = 'Unknown/Not Specified'\n",
    "    report_lines.append(\"Warning: 'intervention' column missing. Cannot categorize interventions.\")\n",
    "\n",
    "# Count frequencies of intervention types\n",
    "intervention_counts = df_obj1['intervention_type'].value_counts()\n",
    "report_lines.append(\"\\nIntervention type distribution:\")\n",
    "for intervention, count in intervention_counts.items():\n",
    "    percentage = (count / len(df_obj1)) * 100\n",
    "    report_lines.append(f\"  {intervention}: {count} studies ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize intervention type frequencies\n",
    "try:\n",
    "    plt.figure(figsize=(10, 6)) # Adjusted size\n",
    "    sns.barplot(x=intervention_counts.values, y=intervention_counts.index, palette='Spectral')\n",
    "    plt.title('Distribution of Intervention Types')\n",
    "    plt.xlabel('Number of Studies')\n",
    "    plt.ylabel('Intervention Type')\n",
    "    plt.tight_layout()\n",
    "    int_type_path = os.path.join(output_dir, 'intervention_types_distribution.png')\n",
    "    plt.savefig(int_type_path, dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    report_lines.append(f\"Intervention type distribution visualization saved to: {int_type_path}\")\n",
    "except Exception as e:\n",
    "    error_msg = f\"Error creating intervention type visualization: {e}\"\n",
    "    print(error_msg)\n",
    "    report_lines.append(error_msg)\n",
    "\n",
    "\n",
    "# Analyze effect sizes by intervention type (if effect size data is available)\n",
    "es_by_intervention_filtered = pd.DataFrame() # Initialize empty dataframe\n",
    "\n",
    "if num_studies_with_es > 0 and 'intervention_type' in df_with_es.columns:\n",
    "    # Add intervention type to the dataframe with effect sizes\n",
    "    df_with_es['intervention_type'] = df_obj1.loc[df_with_es.index, 'intervention_type']\n",
    "\n",
    "    # Group by intervention type and calculate aggregate stats for mean absolute effect size\n",
    "    es_by_intervention = df_with_es.groupby('intervention_type')['mean_effect_size'].agg(\n",
    "        ['mean', 'median', 'std', 'count']\n",
    "    ).dropna(subset=['mean']) # Drop groups where mean couldn't be calculated\n",
    "\n",
    "    es_by_intervention = es_by_intervention.sort_values('mean', ascending=False)\n",
    "\n",
    "    # Filter to include intervention types with a minimum number of studies for reliability\n",
    "    min_studies_per_type = 2 # Adjustable threshold (e.g., 2 or 3)\n",
    "    es_by_intervention_filtered = es_by_intervention[es_by_intervention['count'] >= min_studies_per_type]\n",
    "\n",
    "    if not es_by_intervention_filtered.empty:\n",
    "        report_lines.append(f\"\\nEffect sizes by intervention type (for types with >= {min_studies_per_type} studies):\")\n",
    "        for intervention, stats_row in es_by_intervention_filtered.iterrows():\n",
    "            report_lines.append(f\"  {intervention}: Mean Abs ES = {stats_row['mean']:.3f} (SD={stats_row['std']:.3f}), n = {int(stats_row['count'])}\")\n",
    "\n",
    "        # Visualize effect sizes by intervention type using boxplot\n",
    "        try:\n",
    "            plt.figure(figsize=(12, 7)) # Adjusted size\n",
    "            # Filter the original df_with_es to include only the types meeting the minimum study count\n",
    "            plot_data = df_with_es[df_with_es['intervention_type'].isin(es_by_intervention_filtered.index)]\n",
    "            sns.boxplot(data=plot_data,\n",
    "                        x='mean_effect_size', y='intervention_type',\n",
    "                        order=es_by_intervention_filtered.index, # Order by mean effect size\n",
    "                        palette='Spectral')\n",
    "            plt.title(f'Mean Absolute Effect Size by Intervention Type (n >= {min_studies_per_type})')\n",
    "            plt.xlabel('Mean Absolute Effect Size')\n",
    "            plt.ylabel('Intervention Type')\n",
    "            plt.axvline(mean_es, color='grey', linestyle='--', label=f'Overall Mean ES ({mean_es:.2f})')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            es_by_int_path = os.path.join(output_dir, 'effect_size_by_intervention.png')\n",
    "            plt.savefig(es_by_int_path, dpi=100, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            report_lines.append(f\"Effect size by intervention visualization saved to: {es_by_int_path}\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error creating ES by intervention visualization: {e}\"\n",
    "            print(error_msg)\n",
    "            report_lines.append(error_msg)\n",
    "\n",
    "        # Perform statistical test (e.g., Kruskal-Wallis) if enough groups meet the criteria\n",
    "        if len(es_by_intervention_filtered) >= 2: # Need at least 2 groups to compare\n",
    "            # Prepare data for the test: list of effect sizes for each intervention type\n",
    "            groups_for_test = [\n",
    "                df_with_es[df_with_es['intervention_type'] == int_type]['mean_effect_size'].dropna().values\n",
    "                for int_type in es_by_intervention_filtered.index\n",
    "            ]\n",
    "            # Ensure all groups have data\n",
    "            groups_for_test = [g for g in groups_for_test if len(g) > 0]\n",
    "\n",
    "            if len(groups_for_test) >= 2:\n",
    "                try:\n",
    "                    stat, p_value = stats.kruskal(*groups_for_test) # Non-parametric test suitable for small/non-normal samples\n",
    "                    report_lines.append(f\"\\nKruskal-Wallis test for differences in mean absolute ES between intervention types:\")\n",
    "                    report_lines.append(f\"  H-statistic: {stat:.3f}, p-value: {p_value:.4f}\")\n",
    "                    if p_value < 0.05:\n",
    "                        report_lines.append(\"  Result: Significant differences detected (p < 0.05). Post-hoc tests would be needed to identify specific pairs.\")\n",
    "                    else:\n",
    "                        report_lines.append(\"  Result: No significant differences detected (p >= 0.05).\")\n",
    "                except Exception as e:\n",
    "                    report_lines.append(f\"  Error performing Kruskal-Wallis test: {str(e)}\")\n",
    "            else:\n",
    "                 report_lines.append(\"\\nInsufficient valid groups for Kruskal-Wallis test.\")\n",
    "    else:\n",
    "        report_lines.append(f\"\\nInsufficient data for comparing effect sizes across intervention types (minimum {min_studies_per_type} studies required per type).\")\n",
    "else:\n",
    "    report_lines.append(\"\\nEffect size analysis by intervention type skipped due to lack of sufficient effect size data.\")\n",
    "\n",
    "\n",
    "# --- 4. Mathematical Domain Analysis ---\n",
    "report_lines.append(\"\\n4. MATHEMATICAL DOMAIN ANALYSIS\")\n",
    "report_lines.append(\"-\" * 40)\n",
    "\n",
    "# Define mathematical domains and associated keywords (Spanish/English)\n",
    "# Expanded and refined keywords\n",
    "math_domains = {\n",
    "    'Numeracy/Number Sense': ['numeracy', 'number sense', 'counting', 'numerical', 'número', 'conteo', 'numérico', 'sentido numérico', 'magnitude comparison', 'subitizing'],\n",
    "    'Arithmetic/Calculation': ['arithmetic', 'calculation', 'addition', 'subtraction', 'multiplication', 'division', 'cálculo', 'suma', 'resta', 'multiplicación', 'división', 'operaciones', 'fluency', 'math facts', 'mental math'],\n",
    "    'Geometry/Spatial': ['geometry', 'spatial', 'shape', 'geometric', 'geometría', 'espacial', 'forma', 'visualization', 'mapping', 'rotation'],\n",
    "    'Algebra/Pre-Algebra': ['algebra', 'equation', 'variable', 'ecuación', 'expresión', 'algebraic thinking', 'patterns', 'functions', 'pre-algebra'],\n",
    "    'Problem Solving': ['problem solving', 'problem-solving', 'resolución de problemas', 'word problems', 'problemas verbales', 'mathematical reasoning', 'heuristic'],\n",
    "    'Computational Thinking': ['computational thinking', 'algorithm', 'programming', 'coding', 'pensamiento computacional', 'algoritmo', 'programación', 'logical thinking', 'debugging', 'decomposition', 'pattern recognition', 'abstraction'],\n",
    "    'Fractions/Decimals/Ratio': ['fraction', 'decimal', 'ratio', 'proportion', 'percent', 'fracción', 'proporción', 'razón', 'porcentaje', 'rational numbers'],\n",
    "    'Measurement': ['measurement', 'measure', 'medición', 'medida', 'units', 'unidades', 'length', 'area', 'volume', 'time', 'money'],\n",
    "    'Data Analysis/Stats/Prob': ['statistic', 'probability', 'data analysis', 'estadística', 'probabilidad', 'datos', 'data handling', 'graph', 'chart', 'gráfica', 'average', 'chance']\n",
    "}\n",
    "\n",
    "def identify_domains(text):\n",
    "    \"\"\"Identifies mathematical domains mentioned in text using keywords.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str) or text.strip() == '':\n",
    "        return []\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    found_domains = set() # Use a set to automatically handle duplicates\n",
    "\n",
    "    for domain, keywords in math_domains.items():\n",
    "        for keyword in keywords:\n",
    "            # Use word boundaries (\\b) to match whole words only\n",
    "            pattern = r'\\b' + re.escape(keyword) + r'\\b'\n",
    "            if re.search(pattern, text_lower):\n",
    "                found_domains.add(domain)\n",
    "                break # Move to the next domain once one keyword is found for the current domain\n",
    "\n",
    "    return sorted(list(found_domains)) # Return sorted list\n",
    "\n",
    "# Apply domain identification to the combined cognitive text (if column exists)\n",
    "if 'cognitive_text' in df_obj1.columns:\n",
    "    df_obj1['identified_domains'] = df_obj1['cognitive_text'].apply(identify_domains)\n",
    "    df_obj1['num_domains'] = df_obj1['identified_domains'].apply(len)\n",
    "else:\n",
    "    df_obj1['identified_domains'] = [[] for _ in range(len(df_obj1))]\n",
    "    df_obj1['num_domains'] = 0\n",
    "    report_lines.append(\"Warning: 'cognitive_text' column missing. Cannot identify mathematical domains.\")\n",
    "\n",
    "# Calculate domain frequencies across all studies\n",
    "all_identified_domains = [domain for sublist in df_obj1['identified_domains'] for domain in sublist]\n",
    "domain_counts = pd.Series(all_identified_domains).value_counts()\n",
    "\n",
    "domain_df = pd.DataFrame({'Domain': domain_counts.index, 'Frequency': domain_counts.values})\n",
    "# domain_df = domain_df.sort_values('Frequency', ascending=False) # Already sorted by value_counts\n",
    "\n",
    "if not domain_df.empty:\n",
    "    report_lines.append(\"\\nMathematical domain frequency across studies:\")\n",
    "    for _, row in domain_df.iterrows():\n",
    "        percentage = (row['Frequency'] / len(df_obj1)) * 100 # Percentage of studies mentioning the domain\n",
    "        report_lines.append(f\"  {row['Domain']}: {row['Frequency']} studies ({percentage:.1f}%)\")\n",
    "\n",
    "    # Visualize domain frequencies\n",
    "    try:\n",
    "        plt.figure(figsize=(12, max(6, len(domain_df) * 0.5))) # Adjust height based on number of domains\n",
    "        sns.barplot(data=domain_df, x='Frequency', y='Domain', palette='Blues_r', orient='h')\n",
    "        plt.title('Frequency of Mathematical Domains Addressed in Studies')\n",
    "        plt.xlabel('Number of Studies')\n",
    "        plt.ylabel('Mathematical Domain')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        domains_path = os.path.join(output_dir, 'mathematical_domains_frequency.png')\n",
    "        plt.savefig(domains_path, dpi=100, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        report_lines.append(f\"Mathematical domains visualization saved to: {domains_path}\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error creating mathematical domains visualization: {e}\"\n",
    "        print(error_msg)\n",
    "        report_lines.append(error_msg)\n",
    "\n",
    "else:\n",
    "    report_lines.append(\"No mathematical domains were identified in the provided text data.\")\n",
    "\n",
    "\n",
    "# --- 5. Text Analysis of Cognitive Outcomes ---\n",
    "report_lines.append(\"\\n5. TEXT ANALYSIS OF COGNITIVE OUTCOMES\")\n",
    "report_lines.append(\"-\" * 40)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesses text for NLP: lowercase, remove punctuation/numbers, tokenize, remove stopwords.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str) or text.strip() == '':\n",
    "        return ''\n",
    "\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Remove punctuation and numbers, keep letters and spaces (incl. Spanish characters)\n",
    "    text = re.sub(r'[^a-záéíóúñü\\s]', '', text)\n",
    "    # 3. Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 4. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 5. Remove stopwords (English and Spanish)\n",
    "    try:\n",
    "        stop_words_en = set(stopwords.words('english'))\n",
    "        stop_words_es = set(stopwords.words('spanish'))\n",
    "        stop_words = stop_words_en.union(stop_words_es)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load stopwords - {e}. Proceeding without stopword removal.\")\n",
    "        stop_words = set()\n",
    "\n",
    "\n",
    "    # Add custom stopwords relevant to reviews/education context\n",
    "    custom_stopwords = {\n",
    "        'use', 'using', 'used', 'study', 'studies', 'group', 'groups', 'result', 'results',\n",
    "        'effect', 'effects', 'significant', 'significantly', 'found', 'showed', 'increased', 'improved',\n",
    "        'uso', 'usar', 'utilizar', 'estudio', 'estudios', 'grupo', 'grupos', 'resultado',\n",
    "        'resultados', 'efecto', 'efectos', 'significativo', 'significativa', 'encontrado',\n",
    "        'mostró', 'incrementado', 'mejorado', 'investigación', 'research', 'analysis', 'análisis',\n",
    "        'intervention', 'control', 'comparison', 'students', 'teachers', 'participants',\n",
    "        'intervención', 'comparación', 'estudiantes', 'profesores', 'participantes',\n",
    "        'digital', 'tool', 'tools', 'technology', 'technologies', 'herramienta', 'herramientas',\n",
    "        'tecnología', 'tecnologías', 'skill', 'skills', 'habilidad', 'habilidades', 'level', 'levels',\n",
    "        'primary', 'education', 'school', 'primaria', 'educación', 'escuela', 'impact', 'influence',\n",
    "        'impacto', 'influencia', 'development', 'desarrollo', 'learning', 'aprendizaje',\n",
    "        # Add short common words often missed\n",
    "        'also', 'however', 'may', 'might', 'could', 'would', 'one', 'two', 'post', 'pre'\n",
    "        }\n",
    "    stop_words = stop_words.union(custom_stopwords)\n",
    "\n",
    "    # 6. Filter tokens: remove stopwords and short tokens\n",
    "    processed_tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Apply text preprocessing to the combined cognitive text (if column exists)\n",
    "valid_texts = []\n",
    "if 'cognitive_text' in df_obj1.columns:\n",
    "    df_obj1['processed_cognitive_text'] = df_obj1['cognitive_text'].apply(preprocess_text)\n",
    "    # Get a list of non-empty processed texts for analysis\n",
    "    valid_texts = df_obj1['processed_cognitive_text'].dropna().tolist()\n",
    "    valid_texts = [text for text in valid_texts if text.strip()] # Ensure no empty strings remain\n",
    "    report_lines.append(f\"Preprocessed cognitive text for {len(valid_texts)} studies.\")\n",
    "else:\n",
    "    df_obj1['processed_cognitive_text'] = ''\n",
    "    report_lines.append(\"Warning: 'cognitive_text' column missing. Skipping text analysis.\")\n",
    "\n",
    "term_importance = pd.DataFrame() # Initialize\n",
    "\n",
    "# Proceed with TF-IDF and Word Cloud only if there's enough processed text data\n",
    "if len(valid_texts) > 3: # Need a few documents for TF-IDF to be meaningful\n",
    "    try:\n",
    "        # --- TF-IDF Analysis ---\n",
    "        # Use uni-grams and bi-grams, limit features to top 50\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=50, ngram_range=(1, 2), stop_words=None) # Stopwords already removed\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(valid_texts)\n",
    "\n",
    "        # Get feature names (terms) and their summed TF-IDF scores (importance)\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        total_importance = tfidf_matrix.sum(axis=0).A1 # .A1 converts matrix row to flat numpy array\n",
    "\n",
    "        # Create a DataFrame for easy sorting and plotting\n",
    "        term_importance = pd.DataFrame({'term': feature_names, 'importance': total_importance})\n",
    "        term_importance = term_importance.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        report_lines.append(\"\\nTop 15 key terms/phrases in cognitive effectiveness descriptions (TF-IDF):\")\n",
    "        for _, row in term_importance.head(15).iterrows():\n",
    "            report_lines.append(f\"  {row['term']} (Score: {row['importance']:.3f})\")\n",
    "\n",
    "        # --- Visualization: Key Terms Bar Chart and Word Cloud ---\n",
    "        fig_text, axs = plt.subplots(2, 1, figsize=(14, 12)) # Adjusted layout\n",
    "        fig_text.suptitle(\"Text Analysis of Cognitive Outcome Descriptions\", fontsize=16)\n",
    "\n",
    "        # Bar chart of top TF-IDF terms\n",
    "        if not term_importance.empty:\n",
    "            sns.barplot(data=term_importance.head(15), x='importance', y='term', palette='viridis', ax=axs[0])\n",
    "            axs[0].set_title('Top 15 Key Terms/Phrases (TF-IDF Importance)')\n",
    "            axs[0].set_xlabel('Total TF-IDF Score')\n",
    "            axs[0].set_ylabel('Term / Phrase')\n",
    "        else:\n",
    "             axs[0].text(0.5, 0.5, 'No terms found', ha='center', va='center')\n",
    "             axs[0].set_title('Top Key Terms/Phrases (TF-IDF Importance)')\n",
    "\n",
    "        # Word cloud\n",
    "        all_processed_text = ' '.join(valid_texts)\n",
    "        if all_processed_text.strip(): # Check if there is any text left after processing\n",
    "            try:\n",
    "                wordcloud = WordCloud(width=1000, height=500, # Larger size\n",
    "                                      background_color='white',\n",
    "                                      colormap='viridis', # Consistent palette\n",
    "                                      max_words=75, # Show more words\n",
    "                                      contour_width=1,\n",
    "                                      contour_color='steelblue',\n",
    "                                      collocations=False, # Avoid generating bi-grams within wordcloud itself\n",
    "                                      random_state=42 # for reproducibility\n",
    "                                      ).generate(all_processed_text)\n",
    "\n",
    "                axs[1].imshow(wordcloud, interpolation='bilinear')\n",
    "                axs[1].axis('off')\n",
    "                axs[1].set_title('Word Cloud of Preprocessed Key Terms')\n",
    "            except Exception as wc_error:\n",
    "                 axs[1].text(0.5, 0.5, f'Word cloud generation failed:\\n{wc_error}', ha='center', va='center')\n",
    "                 axs[1].set_title('Word Cloud (Error)')\n",
    "                 axs[1].axis('off')\n",
    "                 print(f\"Word cloud generation failed: {wc_error}\")\n",
    "        else:\n",
    "            axs[1].text(0.5, 0.5, 'Insufficient unique text data\\n after preprocessing for word cloud',\n",
    "                       ha='center', va='center', fontsize=12)\n",
    "            axs[1].set_title('Word Cloud (Insufficient Data)')\n",
    "            axs[1].axis('off')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        text_analysis_path = os.path.join(output_dir, 'cognitive_text_analysis.png')\n",
    "        plt.savefig(text_analysis_path, dpi=100, bbox_inches='tight')\n",
    "        plt.close(fig_text)\n",
    "        report_lines.append(f\"Text analysis visualization saved to: {text_analysis_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error during text analysis (TF-IDF/WordCloud): {str(e)}\"\n",
    "        print(error_msg)\n",
    "        report_lines.append(error_msg)\n",
    "else:\n",
    "    report_lines.append(\"Insufficient valid text data (need >3 studies with cognitive descriptions) for meaningful NLP analysis (TF-IDF, Word Cloud).\")\n",
    "\n",
    "# --- 6. Relationship Network Analysis (Intervention Type <-> Math Domain) ---\n",
    "report_lines.append(\"\\n6. RELATIONSHIP NETWORK ANALYSIS\")\n",
    "report_lines.append(\"-\" * 40)\n",
    "\n",
    "co_occurrences = []\n",
    "pair_frequencies = pd.Series(dtype=int) # Initialize empty Series\n",
    "G = nx.Graph() # Initialize empty graph\n",
    "\n",
    "# Check if necessary columns exist\n",
    "if 'intervention_type' in df_obj1.columns and 'identified_domains' in df_obj1.columns:\n",
    "    # Create pairs of (intervention_type, domain) for each study\n",
    "    for _, row in df_obj1.iterrows():\n",
    "        intervention = row['intervention_type']\n",
    "        domains = row['identified_domains']\n",
    "\n",
    "        # Include pairs only if intervention is known and domains were identified\n",
    "        if intervention != 'Unknown/Not Specified' and domains:\n",
    "            for domain in domains:\n",
    "                # Ensure domain is a non-empty string\n",
    "                if isinstance(domain, str) and domain.strip():\n",
    "                     co_occurrences.append((intervention, domain))\n",
    "\n",
    "    # Count the frequency of each unique (intervention, domain) pair\n",
    "    if co_occurrences:\n",
    "        pair_frequencies = pd.Series(co_occurrences).value_counts()\n",
    "\n",
    "        report_lines.append(\"\\nMost frequent intervention-domain relationships (co-occurrence count):\")\n",
    "        for (intervention, domain), freq in pair_frequencies.head(10).items():\n",
    "            report_lines.append(f\"  {intervention} → {domain}: {freq} studies\")\n",
    "\n",
    "        # --- Create and Visualize the Network Graph ---\n",
    "        # Add nodes: interventions and domains\n",
    "        interventions_in_network = set(pair[0] for pair in pair_frequencies.index)\n",
    "        domains_in_network = set(pair[1] for pair in pair_frequencies.index)\n",
    "\n",
    "        for intervention in interventions_in_network:\n",
    "            G.add_node(intervention, type='Intervention', bipartite=0) # Add attributes for coloring/layout\n",
    "\n",
    "        for domain in domains_in_network:\n",
    "            G.add_node(domain, type='Domain', bipartite=1)\n",
    "\n",
    "        # Add edges with weights based on co-occurrence frequency\n",
    "        for (intervention, domain), weight in pair_frequencies.items():\n",
    "            # Add edge only if both nodes exist (safety check)\n",
    "            if G.has_node(intervention) and G.has_node(domain):\n",
    "                G.add_edge(intervention, domain, weight=weight)\n",
    "\n",
    "        if G.number_of_nodes() > 0 and G.number_of_edges() > 0:\n",
    "            try:\n",
    "                plt.figure(figsize=(18, 14)) # Larger figure for clarity\n",
    "\n",
    "                # Use a layout algorithm suitable for bipartite graphs if desired, or spring layout\n",
    "                # pos = nx.bipartite_layout(G, interventions_in_network)\n",
    "                pos = nx.spring_layout(G, k=0.6, iterations=70, seed=42) # Spring layout often works well visually\n",
    "\n",
    "                # Node styling\n",
    "                node_colors = ['skyblue' if G.nodes[n]['type'] == 'Intervention' else 'lightcoral' for n in G.nodes()]\n",
    "                # Size nodes based on degree (number of connections), with a base size\n",
    "                node_degrees = dict(G.degree())\n",
    "                node_sizes = [(node_degrees.get(n, 0) * 100) + 500 for n in G.nodes()] # Adjust multiplier as needed\n",
    "\n",
    "                # Edge styling\n",
    "                # Normalize edge weights for better visualization if weights vary widely\n",
    "                max_weight = max(nx.get_edge_attributes(G, 'weight').values()) if G.edges else 1\n",
    "                edge_widths = [G.edges[u, v]['weight'] / max_weight * 5 + 0.5 for u, v in G.edges()] # Adjust multiplier/base\n",
    "\n",
    "                # Draw the network components\n",
    "                nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.9)\n",
    "                nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.5, edge_color='grey')\n",
    "                nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "\n",
    "                plt.title('Network of Co-occurrence: Intervention Types and Mathematical Domains', size=18)\n",
    "\n",
    "                # Create legend handles manually\n",
    "                intervention_patch = plt.Line2D([], [], marker='o', markersize=10, linewidth=0, color='skyblue', label='Intervention Type')\n",
    "                domain_patch = plt.Line2D([], [], marker='o', markersize=10, linewidth=0, color='lightcoral', label='Mathematical Domain')\n",
    "                plt.legend(handles=[intervention_patch, domain_patch], loc='upper right', fontsize=12)\n",
    "\n",
    "                plt.axis('off') # Hide axes\n",
    "                plt.tight_layout()\n",
    "\n",
    "                network_path = os.path.join(output_dir, 'intervention_domain_network.png')\n",
    "                plt.savefig(network_path, dpi=100, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                report_lines.append(f\"Relationship network visualization saved to: {network_path}\")\n",
    "\n",
    "                # --- Network Metrics ---\n",
    "                report_lines.append(\"\\nNetwork Analysis Metrics:\")\n",
    "\n",
    "                # Degree Centrality (most connected nodes)\n",
    "                degree_centrality = nx.degree_centrality(G)\n",
    "                # Sort by centrality score, descending\n",
    "                sorted_centrality = sorted(degree_centrality.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "                report_lines.append(\"  Most connected nodes (Normalized Degree Centrality):\")\n",
    "                for node, centrality in sorted_centrality[:5]: # Top 5\n",
    "                     node_type = G.nodes[node]['type']\n",
    "                     report_lines.append(f\"    - {node} ({node_type}): {centrality:.3f}\")\n",
    "\n",
    "                # Betweenness Centrality (nodes acting as bridges) - calculated only if graph is large enough\n",
    "                if G.number_of_nodes() > 2:\n",
    "                    try:\n",
    "                        betweenness = nx.betweenness_centrality(G, normalized=True)\n",
    "                        sorted_betweenness = sorted(betweenness.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "                        report_lines.append(\"  Key bridge nodes (Normalized Betweenness Centrality):\")\n",
    "                        # Report top nodes with non-zero betweenness\n",
    "                        reported_count = 0\n",
    "                        for node, score in sorted_betweenness:\n",
    "                            if score > 1e-6 and reported_count < 5: # Check for non-negligible score, limit to top 5\n",
    "                                node_type = G.nodes[node]['type']\n",
    "                                report_lines.append(f\"    - {node} ({node_type}): {score:.3f}\")\n",
    "                                reported_count += 1\n",
    "                        if reported_count == 0:\n",
    "                            report_lines.append(\"    - No significant bridge nodes found.\")\n",
    "                    except Exception as e:\n",
    "                        report_lines.append(f\"    - Error calculating betweenness centrality: {e}\")\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error creating network visualization or calculating metrics: {e}\"\n",
    "                print(error_msg)\n",
    "                report_lines.append(error_msg)\n",
    "        else:\n",
    "             report_lines.append(\"Network graph could not be generated (no nodes or edges).\")\n",
    "    else:\n",
    "        report_lines.append(\"No co-occurrences found between known intervention types and identified domains. Skipping network analysis.\")\n",
    "else:\n",
    "    report_lines.append(\"Skipping Relationship Network Analysis because 'intervention_type' or 'identified_domains' columns are missing.\")\n",
    "\n",
    "\n",
    "# --- 7. Time Trend Analysis ---\n",
    "report_lines.append(\"\\n7. TEMPORAL TREND ANALYSIS\")\n",
    "report_lines.append(\"-\" * 40)\n",
    "\n",
    "yearly_metrics = pd.DataFrame() # Initialize\n",
    "\n",
    "# Check if 'year' column exists and is numeric\n",
    "if 'year' in df_obj1.columns and pd.api.types.is_numeric_dtype(df_obj1['year']):\n",
    "    # Define aggregations\n",
    "    agg_dict = {\n",
    "        'study_count': ('year', 'size'), # Count studies per year\n",
    "    }\n",
    "    # Add optional aggregations if columns exist\n",
    "    if 'numeric_sample_size' in df_obj1.columns:\n",
    "        agg_dict['mean_sample_size'] = ('numeric_sample_size', 'mean')\n",
    "    if 'num_domains' in df_obj1.columns:\n",
    "         agg_dict['mean_domains_per_study'] = ('num_domains', 'mean')\n",
    "\n",
    "    yearly_metrics = df_obj1.groupby('year').agg(**agg_dict)\n",
    "\n",
    "    # Add mean effect size per year if available\n",
    "    if num_studies_with_es > 0 and 'year' in df_with_es.columns:\n",
    "        yearly_es = df_with_es.groupby('year')['mean_effect_size'].agg(['mean', 'count'])\n",
    "        yearly_es.rename(columns={'mean': 'mean_effect_size', 'count': 'es_study_count'}, inplace=True)\n",
    "        yearly_metrics = yearly_metrics.join(yearly_es, how='left') # Left join to keep all years\n",
    "\n",
    "    # Ensure index is sorted\n",
    "    yearly_metrics.sort_index(inplace=True)\n",
    "\n",
    "    report_lines.append(\"\\nTemporal distribution of studies:\")\n",
    "    for year, row in yearly_metrics.iterrows():\n",
    "        report_lines.append(f\"  {int(year)}: {int(row['study_count'])} studies\")\n",
    "        if 'mean_sample_size' in row and pd.notna(row['mean_sample_size']):\n",
    "             report_lines[-1] += f\", Mean Sample Size: {row['mean_sample_size']:.1f}\"\n",
    "        if 'mean_effect_size' in row and pd.notna(row['mean_effect_size']):\n",
    "             report_lines[-1] += f\", Mean Abs ES: {row['mean_effect_size']:.3f} (n={int(row.get('es_study_count',0))})\"\n",
    "\n",
    "\n",
    "    # Visualize trends if there are enough years of data\n",
    "    if len(yearly_metrics) > 2:\n",
    "        try:\n",
    "            # Determine number of plots needed\n",
    "            plot_count = 1 # Always plot study count\n",
    "            if 'mean_effect_size' in yearly_metrics.columns and yearly_metrics['mean_effect_size'].notna().sum() > 1:\n",
    "                plot_count = 2\n",
    "\n",
    "            fig_trends, axs = plt.subplots(plot_count, 1, figsize=(14, 6 * plot_count), sharex=True) # Share X axis\n",
    "            if plot_count == 1: # If only one plot, axs is not an array\n",
    "                axs = [axs]\n",
    "            fig_trends.suptitle(\"Temporal Trends in Interactive Math Tools Research\", fontsize=16, y=0.99)\n",
    "\n",
    "            # Plot 1: Studies per year (Bar chart)\n",
    "            axs[0].bar(yearly_metrics.index, yearly_metrics['study_count'], color='steelblue')\n",
    "            axs[0].set_title('Number of Studies Published per Year')\n",
    "            # axs[0].set_xlabel('Year') # X label only on bottom plot\n",
    "            axs[0].set_ylabel('Number of Studies')\n",
    "            axs[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            # Add a simple trend line for study count\n",
    "            if len(yearly_metrics) > 2:\n",
    "                 x_years = np.array(yearly_metrics.index)\n",
    "                 y_counts = np.array(yearly_metrics['study_count'])\n",
    "                 z_counts = np.polyfit(x_years, y_counts, 1)\n",
    "                 p_counts = np.poly1d(z_counts)\n",
    "                 axs[0].plot(x_years, p_counts(x_years), \"r--\", alpha=0.6, label=f\"Trend (Slope={z_counts[0]:.2f})\")\n",
    "                 axs[0].legend()\n",
    "\n",
    "\n",
    "            # Plot 2: Mean Effect Size trend over time (if available)\n",
    "            z_es = None # Initialize slope variable\n",
    "            if plot_count == 2:\n",
    "                years_with_es = yearly_metrics[yearly_metrics['mean_effect_size'].notna()]\n",
    "                if len(years_with_es) > 1:\n",
    "                    axs[1].plot(years_with_es.index, years_with_es['mean_effect_size'],\n",
    "                              marker='o', linestyle='-', color='darkred', linewidth=2, label='Mean Abs ES')\n",
    "                    axs[1].set_title('Trend of Mean Absolute Effect Size per Year')\n",
    "                    axs[1].set_xlabel('Year')\n",
    "                    axs[1].set_ylabel('Mean Absolute Effect Size')\n",
    "                    axs[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "                    # Add linear trendline if enough data points (>2)\n",
    "                    if len(years_with_es) > 2:\n",
    "                        x_es_years = np.array(years_with_es.index)\n",
    "                        y_es_means = np.array(years_with_es['mean_effect_size'])\n",
    "                        z_es = np.polyfit(x_es_years, y_es_means, 1) # Calculate slope and intercept\n",
    "                        p_es = np.poly1d(z_es)\n",
    "                        axs[1].plot(x_es_years, p_es(x_es_years), \"r--\", alpha=0.7, label=f\"Trend (Slope={z_es[0]:.3f})\")\n",
    "                        axs[1].legend()\n",
    "\n",
    "                        # Report trend information based on slope\n",
    "                        report_lines.append(f\"\\nMean Absolute Effect Size trend over time (linear fit): Slope = {z_es[0]:.3f}\")\n",
    "                        if abs(z_es[0]) < 0.005: # Threshold for \"stable\"\n",
    "                            trend_direction = \"relatively stable\"\n",
    "                        elif z_es[0] > 0:\n",
    "                            trend_direction = \"increasing\"\n",
    "                        else:\n",
    "                            trend_direction = \"decreasing\"\n",
    "                        report_lines.append(f\"  Overall direction: {trend_direction}\")\n",
    "                    else:\n",
    "                        axs[1].text(0.5, 0.5, 'Need >2 years with ES data for trendline',\n",
    "                                  ha='center', va='center', transform=axs[1].transAxes)\n",
    "                        axs[1].legend()\n",
    "                else:\n",
    "                     axs[1].text(0.5, 0.5, 'Insufficient effect size data points (<2) for trend plot',\n",
    "                               ha='center', va='center', transform=axs[1].transAxes)\n",
    "                     axs[1].set_title('Trend of Mean Absolute Effect Size per Year')\n",
    "                     axs[1].set_xlabel('Year')\n",
    "\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to prevent title overlap\n",
    "            trends_path = os.path.join(output_dir, 'temporal_trends.png')\n",
    "            plt.savefig(trends_path, dpi=100, bbox_inches='tight')\n",
    "            plt.close(fig_trends)\n",
    "            report_lines.append(f\"Temporal trend visualization saved to: {trends_path}\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error creating temporal trends visualization: {e}\"\n",
    "            print(error_msg)\n",
    "            report_lines.append(error_msg)\n",
    "    else:\n",
    "        report_lines.append(\"Insufficient temporal data (<= 2 years) for meaningful trend visualization.\")\n",
    "else:\n",
    "    report_lines.append(\"Temporal Trend Analysis skipped because 'year' column is missing or not numeric.\")\n",
    "\n",
    "\n",
    "# --- 8. Summary Dashboard ---\n",
    "report_lines.append(\"\\n8. SUMMARY DASHBOARD\")\n",
    "report_lines.append(\"-\" * 40)\n",
    "print(\"Generating Summary Dashboard...\")\n",
    "\n",
    "try:\n",
    "    # Create a dashboard figure with a grid layout\n",
    "    fig_dash = plt.figure(figsize=(22, 18)) # Larger figure for dashboard\n",
    "    # GridSpec: 3 rows, 3 columns. Network plot spans bottom row.\n",
    "    gs = fig_dash.add_gridspec(3, 3, height_ratios=[1, 1, 1.5], hspace=0.45, wspace=0.35)\n",
    "    fig_dash.suptitle('Interactive Digital Tools in Math Education: Key Findings Summary', fontsize=24, y=0.98)\n",
    "\n",
    "    # --- Panel 1: Top Intervention Types (Top Left) ---\n",
    "    ax1 = fig_dash.add_subplot(gs[0, 0])\n",
    "    if not intervention_counts.empty:\n",
    "        intervention_counts_sorted = intervention_counts.nlargest(6) # Show top 6\n",
    "        sns.barplot(x=intervention_counts_sorted.values, y=intervention_counts_sorted.index, palette='Blues', ax=ax1, orient='h')\n",
    "        ax1.set_title('Top Intervention Types Used', fontsize=14)\n",
    "        ax1.set_xlabel('Number of Studies', fontsize=12)\n",
    "        ax1.tick_params(axis='y', labelsize=10)\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'No intervention data', ha='center', va='center')\n",
    "        ax1.set_title('Top Intervention Types Used', fontsize=14)\n",
    "    ax1.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "    # --- Panel 2: Studies by Year (Top Middle) ---\n",
    "    ax2 = fig_dash.add_subplot(gs[0, 1])\n",
    "    if not yearly_metrics.empty and 'study_count' in yearly_metrics.columns:\n",
    "        year_counts = yearly_metrics['study_count']\n",
    "        ax2.bar(year_counts.index, year_counts.values, color='steelblue')\n",
    "        ax2.set_title('Studies Published by Year', fontsize=14)\n",
    "        ax2.set_xlabel('Year', fontsize=12)\n",
    "        ax2.set_ylabel('Number of Studies', fontsize=12)\n",
    "        # Optional: add trend line if desired\n",
    "        if len(year_counts) > 2:\n",
    "             x_yr = np.array(year_counts.index)\n",
    "             y_ct = np.array(year_counts.values)\n",
    "             z_ct = np.polyfit(x_yr, y_ct, 1)\n",
    "             p_ct = np.poly1d(z_ct)\n",
    "             ax2.plot(x_yr, p_ct(x_yr), \"r--\", alpha=0.7)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No year data', ha='center', va='center')\n",
    "        ax2.set_title('Studies Published by Year', fontsize=14)\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "    # --- Panel 3: Top Mathematical Domains (Top Right) ---\n",
    "    ax3 = fig_dash.add_subplot(gs[0, 2])\n",
    "    if not domain_df.empty:\n",
    "        top_domains = domain_df.head(6) # Show top 6\n",
    "        sns.barplot(x='Frequency', y='Domain', data=top_domains, palette='Greens', ax=ax3, orient='h')\n",
    "        ax3.set_title('Top Mathematical Domains Addressed', fontsize=14)\n",
    "        ax3.set_xlabel('Number of Studies', fontsize=12)\n",
    "        ax3.tick_params(axis='y', labelsize=10)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'No domain data available', ha='center', va='center')\n",
    "        ax3.set_title('Top Mathematical Domains Addressed', fontsize=14)\n",
    "    ax3.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "    # --- Panel 4: Effect Size Distribution (Middle Left) ---\n",
    "    ax4 = fig_dash.add_subplot(gs[1, 0])\n",
    "    if num_studies_with_es > 0 and 'effect_size_category' in df_with_es.columns:\n",
    "        # Use the previously calculated category counts\n",
    "        if 'es_category_counts' in locals() and not es_category_counts.empty:\n",
    "             sns.barplot(x=es_category_counts.index, y=es_category_counts.values, palette='Oranges', ax=ax4)\n",
    "             ax4.set_title('Distribution of Effect Size Magnitudes', fontsize=14)\n",
    "             ax4.set_xlabel('Effect Size Category (Mean Abs ES)', fontsize=12)\n",
    "             ax4.set_ylabel('Number of Studies', fontsize=12)\n",
    "             ax4.tick_params(axis='x', rotation=25, labelsize=10) # Rotate labels slightly\n",
    "        else:\n",
    "             ax4.text(0.5, 0.5, 'Categorization failed', ha='center', va='center')\n",
    "             ax4.set_title('Distribution of Effect Size Magnitudes', fontsize=14)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No effect size data available', ha='center', va='center')\n",
    "        ax4.set_title('Distribution of Effect Size Magnitudes', fontsize=14)\n",
    "    ax4.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "    # --- Panel 5: Effect Size by Intervention Type (Middle Middle) ---\n",
    "    ax5 = fig_dash.add_subplot(gs[1, 1])\n",
    "    # Use the filtered data calculated earlier (es_by_intervention_filtered)\n",
    "    if not es_by_intervention_filtered.empty:\n",
    "        # Plot mean effect size for types with enough studies\n",
    "        data_to_plot = es_by_intervention_filtered.reset_index()\n",
    "        sns.barplot(\n",
    "            x='mean', # Mean absolute effect size\n",
    "            y='intervention_type',\n",
    "            data=data_to_plot,\n",
    "            palette='Reds',\n",
    "            ax=ax5,\n",
    "            orient='h'\n",
    "        )\n",
    "        # Add error bars if std is available\n",
    "        if 'std' in data_to_plot.columns:\n",
    "             ax5.errorbar(x=data_to_plot['mean'], y=data_to_plot['intervention_type'],\n",
    "                          xerr=data_to_plot['std'], fmt='none', c='black', capsize=3, alpha=0.6)\n",
    "\n",
    "        ax5.set_title(f'Mean Abs ES by Intervention (n>={min_studies_per_type})', fontsize=14)\n",
    "        ax5.set_xlabel('Mean Absolute Effect Size', fontsize=12)\n",
    "        ax5.set_ylabel('') # Remove y-label for cleaner look\n",
    "        ax5.tick_params(axis='y', labelsize=10)\n",
    "        ax5.axvline(mean_es, color='grey', linestyle='--', label=f'Overall Mean ({mean_es:.2f})')\n",
    "        ax5.legend(fontsize=10)\n",
    "    else:\n",
    "        ax5.text(0.5, 0.5, 'Insufficient data for\\nES by intervention comparison', ha='center', va='center')\n",
    "        ax5.set_title('Mean Abs ES by Intervention', fontsize=14)\n",
    "    ax5.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "    # --- Panel 6: Word Cloud of Key Terms (Middle Right) ---\n",
    "    ax6 = fig_dash.add_subplot(gs[1, 2])\n",
    "    # Use the text processed earlier\n",
    "    all_processed_text_dash = ' '.join(valid_texts)\n",
    "    if all_processed_text_dash.strip():\n",
    "        try:\n",
    "            wordcloud_dash = WordCloud(width=400, height=300,\n",
    "                                   background_color='white',\n",
    "                                   colormap='viridis',\n",
    "                                   max_words=40, # Fewer words for small panel\n",
    "                                   contour_width=1,\n",
    "                                   collocations=False,\n",
    "                                   random_state=42\n",
    "                                   ).generate(all_processed_text_dash)\n",
    "            ax6.imshow(wordcloud_dash, interpolation='bilinear')\n",
    "            ax6.set_title('Key Terms in Cognitive Outcomes', fontsize=14)\n",
    "        except Exception as e:\n",
    "             ax6.text(0.5, 0.5, f'WordCloud Error:\\n{e}', ha='center', va='center', fontsize=9)\n",
    "             ax6.set_title('Key Terms (Error)', fontsize=14)\n",
    "    else:\n",
    "        ax6.text(0.5, 0.5, 'Insufficient text data', ha='center', va='center')\n",
    "        ax6.set_title('Key Terms in Cognitive Outcomes', fontsize=14)\n",
    "    ax6.axis('off')\n",
    "\n",
    "\n",
    "    # --- Panel 7: Simplified Network Visualization (Bottom Row, Spanning All Columns) ---\n",
    "    ax7 = fig_dash.add_subplot(gs[2, :])\n",
    "    # Use the graph G created earlier\n",
    "    if G.number_of_nodes() > 1 and G.number_of_edges() > 0:\n",
    "        try:\n",
    "            # Use a layout suitable for the potentially smaller space\n",
    "            pos_dash = nx.spring_layout(G, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "            # Node styling (similar to previous network plot, maybe smaller sizes)\n",
    "            node_colors_dash = ['skyblue' if G.nodes[n]['type'] == 'Intervention' else 'lightcoral' for n in G.nodes()]\n",
    "            node_degrees_dash = dict(G.degree())\n",
    "            node_sizes_dash = [(node_degrees_dash.get(n, 0) * 60) + 250 for n in G.nodes()] # Smaller sizes\n",
    "\n",
    "            # Edge styling (similar to previous, maybe thinner lines)\n",
    "            max_weight_dash = max(nx.get_edge_attributes(G, 'weight').values()) if G.edges else 1\n",
    "            edge_widths_dash = [G.edges[u, v]['weight'] / max_weight_dash * 3 + 0.3 for u, v in G.edges()]\n",
    "\n",
    "            # Draw network\n",
    "            nx.draw_networkx_nodes(G, pos_dash, node_size=node_sizes_dash, node_color=node_colors_dash, alpha=0.85, ax=ax7)\n",
    "            nx.draw_networkx_edges(G, pos_dash, width=edge_widths_dash, alpha=0.5, edge_color='gray', ax=ax7)\n",
    "            nx.draw_networkx_labels(G, pos_dash, font_size=9, font_weight='normal', ax=ax7) # Slightly smaller font\n",
    "\n",
    "            ax7.set_title('Relationship Network: Intervention Types and Mathematical Domains', size=16)\n",
    "            # Add legend (re-using handles from earlier)\n",
    "            if 'intervention_patch' in locals() and 'domain_patch' in locals():\n",
    "                 ax7.legend(handles=[intervention_patch, domain_patch], loc='upper right', fontsize=11)\n",
    "            ax7.axis('off')\n",
    "\n",
    "        except Exception as e:\n",
    "             ax7.text(0.5, 0.5, f'Network plot error: {e}', ha='center', va='center', fontsize=12)\n",
    "             ax7.set_title('Relationship Network (Error)', size=16)\n",
    "             ax7.axis('off')\n",
    "    else:\n",
    "        ax7.text(0.5, 0.5, 'Insufficient data for network visualization', ha='center', va='center', fontsize=14)\n",
    "        ax7.set_title('Relationship Network', size=16)\n",
    "        ax7.axis('off')\n",
    "\n",
    "    # --- Final Adjustments and Saving ---\n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.96]) # Adjust layout rect to prevent overlap\n",
    "    dashboard_path = os.path.join(output_dir, 'summary_dashboard.png')\n",
    "    plt.savefig(dashboard_path, dpi=120, bbox_inches='tight') # Higher DPI for dashboard\n",
    "    plt.close(fig_dash)\n",
    "    report_lines.append(f\"Summary dashboard visualization saved to: {dashboard_path}\")\n",
    "    print(\"Summary Dashboard generated successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = f\"FATAL ERROR generating the summary dashboard: {e}\"\n",
    "    print(error_msg)\n",
    "    report_lines.append(f\"\\n!! ERROR: Summary dashboard could not be generated due to: {e} !!\")\n",
    "\n",
    "\n",
    "# --- 9. Key Findings and Conclusions Synthesis ---\n",
    "report_lines.append(\"\\n9. KEY FINDINGS AND CONCLUSIONS\")\n",
    "report_lines.append(\"-\" * 40)\n",
    "\n",
    "# Summarize key findings based on the analyses performed\n",
    "total_studies_analyzed = len(df_obj1)\n",
    "report_lines.append(f\"\\nThis analysis examined {total_studies_analyzed} studies focusing on the impact of interactive digital tools\")\n",
    "report_lines.append(f\"on mathematical skills and computational thinking in primary education.\")\n",
    "if min_year and max_year:\n",
    "    report_lines.append(f\"The studies cover the period from {min_year} to {max_year}.\")\n",
    "\n",
    "# --- Effectiveness Findings (based on Effect Sizes) ---\n",
    "report_lines.append(\"\\n1. Effectiveness of Interactive Digital Tools:\")\n",
    "if num_studies_with_es > 0 and not np.isnan(mean_es):\n",
    "    report_lines.append(f\"  - {num_studies_with_es} out of {total_studies_analyzed} studies ({num_studies_with_es/total_studies_analyzed:.1%}) provided extractable effect sizes.\")\n",
    "    report_lines.append(f\"  - The average absolute effect size across these studies was {mean_es:.3f} (Median: {median_es:.3f}).\")\n",
    "\n",
    "    # Interpret overall effect size magnitude\n",
    "    if mean_es < 0.2: effect_interpretation = \"small\"\n",
    "    elif mean_es < 0.5: effect_interpretation = \"medium\"\n",
    "    elif mean_es < 0.8: effect_interpretation = \"large\"\n",
    "    else: effect_interpretation = \"very large\"\n",
    "    report_lines.append(f\"  - This suggests an overall {effect_interpretation} positive impact on average.\")\n",
    "\n",
    "    # Summary based on categories\n",
    "    if 'es_category_counts' in locals() and not es_category_counts.empty:\n",
    "        main_category = es_category_counts.idxmax()\n",
    "        main_percentage = (es_category_counts.max() / es_category_counts.sum()) * 100\n",
    "        report_lines.append(f\"  - The most common effect size magnitude found was '{main_category}', observed in {main_percentage:.1f}% of studies with ES data.\")\n",
    "\n",
    "    # Most effective intervention types based on ES\n",
    "    if not es_by_intervention_filtered.empty:\n",
    "         # Report top 1-2 based on mean ES\n",
    "         top_interventions = es_by_intervention_filtered.head(2)\n",
    "         report_lines.append(f\"  - Intervention types showing the highest average effectiveness (min. {min_studies_per_type} studies):\")\n",
    "         for int_name, stats in top_interventions.iterrows():\n",
    "              report_lines.append(f\"      * {int_name} (Mean Abs ES = {stats['mean']:.3f}, n={int(stats['count'])})\")\n",
    "    else:\n",
    "         report_lines.append(\"  - Insufficient data to reliably compare effectiveness across different intervention types.\")\n",
    "else:\n",
    "    report_lines.append(f\"  - Insufficient or no extractable effect size data was found across the {total_studies_analyzed} studies to quantify overall effectiveness robustly.\")\n",
    "    report_lines.append(f\"  - Only {num_studies_with_es} studies ({num_studies_with_es/total_studies_analyzed:.1%}) had any potential ES values.\")\n",
    "\n",
    "\n",
    "# --- Intervention Types Findings ---\n",
    "report_lines.append(\"\\n2. Predominant Intervention Types:\")\n",
    "if not intervention_counts.empty:\n",
    "    top_intervention_type = intervention_counts.index[0]\n",
    "    top_intervention_count = intervention_counts.iloc[0]\n",
    "    top_intervention_percentage = (top_intervention_count / total_studies_analyzed) * 100\n",
    "    report_lines.append(f\"  - The most frequently studied intervention type was '{top_intervention_type}' ({top_intervention_count} studies, {top_intervention_percentage:.1f}%).\")\n",
    "    report_lines.append(f\"  - A total of {len(intervention_counts)} distinct intervention categories were identified.\")\n",
    "    # Mention least common types if space/interest\n",
    "    least_common = intervention_counts.tail(min(3, len(intervention_counts))).index.tolist()\n",
    "    report_lines.append(f\"  - Less common intervention types included: {', '.join(least_common)}.\")\n",
    "else:\n",
    "     report_lines.append(\"  - Intervention types could not be determined (missing data or categorization failed).\")\n",
    "\n",
    "\n",
    "# --- Mathematical Domain Findings ---\n",
    "report_lines.append(\"\\n3. Mathematical Domains Focused Upon:\")\n",
    "if not domain_df.empty:\n",
    "    top_domain = domain_df.iloc[0]['Domain']\n",
    "    top_domain_count = domain_df.iloc[0]['Frequency']\n",
    "    top_domain_percentage = (top_domain_count / total_studies_analyzed) * 100 # % of studies covering this domain\n",
    "    report_lines.append(f\"  - The most frequently addressed mathematical domain was '{top_domain}' (covered in {top_domain_count} studies, {top_domain_percentage:.1f}%).\")\n",
    "\n",
    "    # Identify less studied domains (potential gaps)\n",
    "    bottom_domains = domain_df.tail(max(0, min(3, len(domain_df)-1)))['Domain'].tolist() # Avoid repeating top if only 1-2 domains\n",
    "    if bottom_domains:\n",
    "         report_lines.append(f\"  - Domains less frequently covered included: {', '.join(bottom_domains)}.\")\n",
    "else:\n",
    "    report_lines.append(\"  - Mathematical domains could not be determined (missing data or identification failed).\")\n",
    "\n",
    "\n",
    "# --- Key Network Relationship Findings ---\n",
    "report_lines.append(\"\\n4. Intervention-Domain Relationships:\")\n",
    "if not pair_frequencies.empty:\n",
    "    top_pair = pair_frequencies.index[0]\n",
    "    top_pair_count = pair_frequencies.iloc[0]\n",
    "    report_lines.append(f\"  - The strongest co-occurrence was between '{top_pair[0]}' interventions and the '{top_pair[1]}' domain ({top_pair_count} studies).\")\n",
    "\n",
    "    # Identify potential gaps from network analysis (e.g., domains with few intervention links)\n",
    "    if G.number_of_nodes() > 0:\n",
    "        domain_nodes = [n for n, attr in G.nodes(data=True) if attr.get('type') == 'Domain']\n",
    "        if domain_nodes:\n",
    "            domain_degrees = {n: G.degree(n) for n in domain_nodes}\n",
    "            sorted_domain_degrees = sorted(domain_degrees.items(), key=lambda item: item[1]) # Sort by degree ascending\n",
    "            least_connected_domains = [d[0] for d in sorted_domain_degrees[:min(3, len(sorted_domain_degrees))] if d[1] <= 1] # Domains connected to 0 or 1 intervention types\n",
    "            if least_connected_domains:\n",
    "                report_lines.append(f\"  - Potential research gaps: Domains like {', '.join(least_connected_domains)} appear to be linked with fewer types of interventions in this dataset.\")\n",
    "            else:\n",
    "                 report_lines.append(\"  - All identified domains were connected to multiple intervention types.\")\n",
    "        else:\n",
    "             report_lines.append(\"  - Could not analyze domain connectivity (no domain nodes in network).\")\n",
    "    else:\n",
    "        report_lines.append(\"  - Network analysis was not performed or failed.\")\n",
    "else:\n",
    "    report_lines.append(\"  - Insufficient data to analyze relationships between intervention types and domains.\")\n",
    "\n",
    "\n",
    "# --- Time Trends Summary ---\n",
    "report_lines.append(\"\\n5. Temporal Trends:\")\n",
    "if not yearly_metrics.empty and len(yearly_metrics) > 2:\n",
    "    # Study Volume Trend (using slope calculated earlier if available)\n",
    "    if 'z_counts' in locals():\n",
    "         if abs(z_counts[0]) < 0.1: volume_trend = \"relatively stable\" # Adjust threshold as needed\n",
    "         elif z_counts[0] > 0: volume_trend = \"an increasing\"\n",
    "         else: volume_trend = \"a decreasing\"\n",
    "         report_lines.append(f\"  - Research publication volume shows {volume_trend} trend over the analyzed period (Slope ≈ {z_counts[0]:.2f} studies/year).\")\n",
    "    else:\n",
    "         report_lines.append(\"  - Trend in research volume could not be determined.\")\n",
    "\n",
    "    # Effect Size Trend (using slope calculated earlier if available)\n",
    "    if 'z_es' in locals() and z_es is not None: # Check if slope was calculated\n",
    "        if abs(z_es[0]) < 0.005: es_trend_direction = \"relatively stable\"\n",
    "        elif z_es[0] > 0.005: es_trend_direction = \"a slight increasing\"\n",
    "        else: es_trend_direction = \"a slight decreasing\"\n",
    "        report_lines.append(f\"  - The average reported effect sizes show {es_trend_direction} trend over time (Slope ≈ {z_es[0]:.3f} ES units/year).\")\n",
    "    elif num_studies_with_es > 0:\n",
    "         report_lines.append(\"  - Trend in effect sizes could not be reliably determined (e.g., insufficient data points).\")\n",
    "    #else: No need to report ES trend if no ES data exists anyway\n",
    "else:\n",
    "    report_lines.append(\"  - Insufficient data spanning multiple years to determine reliable temporal trends.\")\n",
    "\n",
    "\n",
    "# --- Methodological Observations ---\n",
    "report_lines.append(\"\\n6. Methodological Observations:\")\n",
    "\n",
    "# Sample Size Analysis\n",
    "if 'numeric_sample_size' in df_obj1.columns and df_obj1['numeric_sample_size'].notna().sum() > 0:\n",
    "    mean_sample = df_obj1['numeric_sample_size'].mean()\n",
    "    median_sample = df_obj1['numeric_sample_size'].median()\n",
    "    report_lines.append(f\"  - Sample sizes varied, with a mean of {mean_sample:.1f} and a median of {median_sample:.0f} participants.\")\n",
    "\n",
    "    # Check for small sample sizes (e.g., < 30)\n",
    "    small_sample_threshold = 30\n",
    "    small_sample_count = (df_obj1['numeric_sample_size'] < small_sample_threshold).sum()\n",
    "    valid_sample_count = df_obj1['numeric_sample_size'].notna().sum()\n",
    "    if valid_sample_count > 0:\n",
    "        small_sample_percent = (small_sample_count / valid_sample_count) * 100\n",
    "        if small_sample_percent > 10: # Report if >10% are small\n",
    "             report_lines.append(f\"  - Approximately {small_sample_percent:.1f}% of studies with sample size data used small samples (< {small_sample_threshold} participants).\")\n",
    "else:\n",
    "    report_lines.append(\"  - Sample size information was largely missing or could not be extracted numerically.\")\n",
    "\n",
    "# Effect Size Reporting Rate\n",
    "es_reporting_rate = (num_studies_with_es / total_studies_analyzed) * 100 if total_studies_analyzed > 0 else 0\n",
    "report_lines.append(f\"  - Extractable effect sizes were reported in only {es_reporting_rate:.1f}% of the analyzed studies.\")\n",
    "if es_reporting_rate < 50:\n",
    "    report_lines.append(\"    - This low reporting rate limits the ability to perform robust meta-analysis.\")\n",
    "\n",
    "# Study Design (Basic Overview - requires 'study_design' column)\n",
    "if 'study_design' in df_obj1.columns:\n",
    "     design_counts = df_obj1['study_design'].value_counts()\n",
    "     if not design_counts.empty:\n",
    "         top_design = design_counts.index[0]\n",
    "         top_design_percent = (design_counts.iloc[0] / total_studies_analyzed) * 100\n",
    "         report_lines.append(f\"  - The most common study design mentioned appeared to be '{top_design}' ({top_design_percent:.1f}%). (Note: Requires consistent terminology in source data).\")\n",
    "     else:\n",
    "         report_lines.append(\"  - Study design information was sparse or highly variable.\")\n",
    "\n",
    "\n",
    "# --- Implications and Recommendations ---\n",
    "report_lines.append(\"\\n7. Implications for Practice and Research:\")\n",
    "\n",
    "# Overall effectiveness conclusion (cautious if ES data is limited)\n",
    "if num_studies_with_es > 0 and not np.isnan(mean_es):\n",
    "    report_lines.append(f\"  - The available evidence suggests interactive digital tools have a generally positive impact (average effect size: {effect_interpretation}, mean abs ES ≈ {mean_es:.3f}) on math skills/CT in primary students.\")\n",
    "else:\n",
    "    report_lines.append(\"  - While many studies exist, the lack of consistent effect size reporting makes it difficult to draw strong conclusions about the overall magnitude of impact.\")\n",
    "\n",
    "# Recommendations based on findings\n",
    "if not es_by_intervention_filtered.empty:\n",
    "     top_performing_intervention = es_by_intervention_filtered.index[0]\n",
    "     report_lines.append(f\"  - Interventions like '{top_performing_intervention}' showed particularly promising results in this dataset and warrant further investigation and potential adoption.\")\n",
    "\n",
    "report_lines.append(\"  - Recommendations for Future Research:\")\n",
    "report_lines.append(\"    * Focus on less-studied mathematical domains (e.g., \" + \", \".join(bottom_domains if 'bottom_domains' in locals() and bottom_domains else ['Domains identified as less frequent']) + \") and intervention types.\")\n",
    "report_lines.append(\"    * Improve methodological rigor, including using larger sample sizes and control groups.\")\n",
    "report_lines.append(\"    * Crucially, consistently report standardized effect sizes (e.g., Cohen's d, Hedges' g) to facilitate meta-analysis.\")\n",
    "report_lines.append(\"    * Conduct longitudinal studies to assess the long-term impact of these tools.\")\n",
    "if not pair_frequencies.empty and 'least_connected_domains' in locals() and least_connected_domains:\n",
    "     report_lines.append(f\"    * Investigate the effectiveness of various intervention types specifically for domains like {', '.join(least_connected_domains)}.\")\n",
    "\n",
    "# --- Limitations ---\n",
    "report_lines.append(\"\\n8. Limitations of This Analysis:\")\n",
    "report_lines.append(f\"  - Based on a limited set of {total_studies_analyzed} studies found in '{data_file}'.\")\n",
    "report_lines.append(\"  - Potential publication bias (studies with positive results may be more likely published).\")\n",
    "report_lines.append(\"  - Heterogeneity in study designs, interventions, outcome measures, and reporting quality limits direct comparability.\")\n",
    "report_lines.append(\"  - Automated extraction of effect sizes, intervention types, and domains relies on keyword matching and may miss nuances or contain inaccuracies.\")\n",
    "report_lines.append(\"  - Categorization of interventions and domains involved simplification.\")\n",
    "report_lines.append(\"  - Quality appraisal of individual studies was not performed in this automated analysis.\")\n",
    "\n",
    "# --- Save the Final Report ---\n",
    "report_file_path = os.path.join(output_dir, 'Objective1_Analysis_Report.txt') # More descriptive filename\n",
    "try:\n",
    "    with open(report_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(report_lines))\n",
    "    print(f\"\\nComprehensive analysis report saved successfully to: {report_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving the final report: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"ANALYSIS COMPLETE. All generated outputs saved in the '{output_dir}' directory.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eb5598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf4dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef68804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EntornoRevSis (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
